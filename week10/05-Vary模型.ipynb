{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bf0817-f439-4283-b8db-54784558176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"Vary-toy/Vary-master/\")\n",
    "os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'\n",
    "\n",
    "# pip install albumentations==1.4.8 albucore==0.0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893d16f5-27e0-4e6b-a598-2d4be633c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.5 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from vary.utils.conversation import conv_templates, SeparatorStyle\n",
    "from vary.utils.utils import disable_torch_init\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\n",
    "from vary.model import *\n",
    "from vary.utils.utils import KeywordsStoppingCriteria\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from vary.model.plug.blip_process import BlipImageEvalProcessor\n",
    "from transformers import TextStreamer\n",
    "from vary.model.plug.transforms import train_transform, test_transform\n",
    "\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = '<imgpad>'\n",
    "DEFAULT_IM_START_TOKEN = '<img>'\n",
    "DEFAULT_IM_END_TOKEN = '</img>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c32e91-6f65-4f9a-960f-b5b70cff6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9f89ca-f1e1-4e68-9c61-bc1fe3ee4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mmgpt to instantiate a model of type vary. This is not supported for all configurations of models and can yield errors.\n",
      "QWenLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "varyQwenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "QWenLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "varyQwenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "QWenLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "varyQwenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "varyQwenForCausalLM(\n",
       "  (transformer): varyQwenModel(\n",
       "    (wte): Embedding(151860, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (w2): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (c_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "    (vision_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (vision_tower_high): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "      (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (mm_projector): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mm_projector_vary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151860, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "disable_torch_init()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/model/Vary-toy\", trust_remote_code=True)\n",
    "model = varyQwenForCausalLM.from_pretrained(\"/root/autodl-tmp/model/Vary-toy\", low_cpu_mem_usage=True, trust_remote_code=True)\n",
    "model.to(device='cuda',  dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c86523-0c59-442f-8bd9-0462913b5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = CLIPImageProcessor.from_pretrained('/root/autodl-tmp/model/clip-vit-large-patch14/', torch_dtype=torch.float16)\n",
    "image_processor_high = BlipImageEvalProcessor(image_size=1024)\n",
    "use_im_start_end = True\n",
    "image_token_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df107ba3-f0e5-4c9f-911e-7c9bf012c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vary(prompt: str, image_path: str):\n",
    "    qs = prompt\n",
    "    if use_im_start_end:\n",
    "        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN*image_token_len + DEFAULT_IM_END_TOKEN  + '\\n' + qs\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n",
    "    \n",
    "    \n",
    "    conv_mode = \"mpt\"\n",
    "    \n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer([prompt])\n",
    "    \n",
    "    image = load_image(image_path)\n",
    "    image_1 = image.copy()\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "    \n",
    "    image_tensor_1 = image_processor_high(image_1)\n",
    "    \n",
    "    input_ids = torch.as_tensor(inputs.input_ids).cuda()\n",
    "    \n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=[(image_tensor.unsqueeze(0).half().cuda(), image_tensor_1.unsqueeze(0).half().cuda())],\n",
    "            do_sample=True,\n",
    "            num_beams = 1,\n",
    "            # temperature=0.2,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=2048,\n",
    "            stopping_criteria=[stopping_criteria]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff984634-a84e-4238-a168-1d0a37d12d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# \n",
      "小结\n",
      "\n",
      "## 向量检索的挑战与实践\n",
      "\n",
      "- 数据规模大, 信息密度高, 处理成本高\n",
      "- 向量检索、RAG 需求快速增长\n",
      "向量检索的新CAP问题\n",
      "\\(\\cdot\\) 在成本、精度、性能之间取舍\n",
      "向量数据库实践\n",
      "- 存储工程与向量索引算法深度优化\n",
      "・通过混合磁盘索引降低成本\n",
      "- 让应用精通向量提升召回率\n",
      "最新学术研究与应用\n",
      "\\(\\cdot\\) RabinQ 超高量化比\n",
      "HGraph层次化索引框架\n",
      "- 提供灵活组合能力，快速构建新索引\n",
      "磁盘索引上的改进\n",
      "- PAGE 进一步降低内存和 \\(I O\\) 需求\n",
      "基于公开 Benchmark 工具的性能调优\n",
      "- GIST-960数据集业界 SOTA\n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Cnvert the image to latex format:\", \"data/iShot_2025-04-23_16.31.41.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "296b5750-36b6-4bb4-acb0-4f35e303015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "建设地点\n",
      "\n",
      "建设规模 \\(1-7 \\mathrm{~h}\\)\n",
      "土方概规模 893m³、合占\\(62\\%\\)\n",
      "\n",
      "土层\n",
      "活动区积沙大放:\n",
      "质来源 电\n",
      "话 (883)一817715\n",
      "控制投资：拟投,\n",
      "建筑面积\n",
      "正常设计依据:（请填报明批文的票要内容及批文号） \n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Cnvert the image to markdown format:\", \"data/xh44z6ajf6noc_22c15d2af82e42a290dac1dde66bc685.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "696809eb-605e-43c9-b1ec-918ff27a752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[720, 126, 938, 635]\n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Detect the person in the image\", \"data/pingpong.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6e7113b-5db0-4752-9277-b306cbe6c342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two people playing ping pong in an indoor court.\n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Describe the image\", \"data/pingpong.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce90c2-f2be-42aa-982a-7a3bc62f34e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
