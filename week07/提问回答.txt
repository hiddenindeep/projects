- 像实体这一部分面试官一般会怎么问呢，可以举些例子吗？
你是一个严格的、专业的大模型、NLP算法工程师。请以实体识别出10个面试题。


- 实体抽取的提问是框架自动完成的吧？
帮我识别XX实体，含义为YY。


- 老师，我就是做信息抽取相关研究的，一直有一些困惑和迷茫：当前工程化的实体关系抽取是不是基本上可以用大模型达到预期？小模型的抽取是不是只有在垂直领域训练下有一定意义？

大模型：
    通用能力、zero shot、拿来就用
    大模型速度慢、不可控、适用于通用文本
BERT：
    精度高、可控、速度快
    需要额外标注、额外训练、特定任务

- 老师这个BIO的bert训练是全量训练把？不是微调BERT吧，

微调 是 在原有预训练模型的基础上训练，可以微调所有层，部分层；

- 老师，实体抽取用QWEN3 0.6B是不是本地部署，速度也可以，用来替代bert?

 QWEN3 0.6B 比 BERT微调 差一些


- 大模型开发分类：提示词、RAG、微调、预训练

部分层冻结（freeze）不更新参数
```
for param in model.parameters():
    param.requires_grad = False

trainable_params = filter(lambda p: p.requires_grad, model.parameters())
optimizer = optim.SGD(trainable_params, lr=0.001, momentum=0.9)
```

假设原始的模型：Qwen3 7B，16GB
任务1 常规微调 -》 16GB权重
任务2 常规微调 -》 16GB权重
任务3 常规微调 -》 16GB权重


假设原始的模型：Qwen3 7B，16GB + LoRA
任务1 常规微调 -》 20MB权重 -》 插件
任务2 常规微调 -》 20MB权重
任务3 常规微调 -》 20MB权重


- Lora 很好用 ？ BERT上呢。 是否也需要Lora呢？

- 实际开发中，如果要为微调的话，是自己写代码微调还是用llamafactory, xtuner这类的工具(UI)微调好一些

llamafactory 上手简单 vs PEFT 可定制化

- 多轮对话里面用到transformer了嘛

transformer -》 gpt -》 chatgpt -》 多轮对话

- lora target_model中，对什么层（q_proj\k_proj）这些都是约定好的吗？怎么知道是哪些参数？

科研人员通过实验发现的，你参考即可。

- 微调好了那怎么对比训练的效果 提升了多少呢

- 老师，LoRA的矩阵A和B是怎么定的呀，怎么保证不同场景不同开发者用的A和B是一样的？

- 微调可能导致负面的效果吗？

知识遗忘

qwen3 （通用模型）  -》 微调 -》 领域模型（遗忘非领域的知识点）


- hf trainer没有可视化功能吗？
可以写tensorboard， wandb 可视化库

- 可以把MOE大模型理解成对各个方向进行微调的多个小模型组合而成的模型吗
大致思路差不多，moe 多了小模型贡献度打分的逻辑。

- 老师每个项目能写个文档对项目结构做介绍嘛
后序补上

- coze / dify 需要自己买个服务器吗？
可以本地部署，私有化部署

- RAG中划分chunck写入到ES时，需要创建索引， 这个索引名称应该是对这个chunck的总结，索引名称需要自己标注吗 ？ 另外划分的chunck是随机的，命令的索引名称对chunck的概括可能不准确，怎么解决
一个知识库的是一个index

- 老师，熟悉Dify/扣子后，是否可以自己单独创业，里面运用的模型是啥？
中小公司

- 那程序实现的多智能体和工作流（langchain/langgraph）和dify/coze相比各有什么优势啊
langchain/langgraph 后端开发 做 智能体，没有界面，更加可靠；
dify/coze 低代码形式 做 智能体，有界面，上手简单 但 性能差；


# 作业1
理解 03_DeepWalk.py 的原理，运行并进行可视化。回答 03_DeepWalk 与 word2vec的关系。

# 作业2
用Qwen-LoRA方法，微调一个识别模型，数据集参考：04_BERT实体抽取.py

# 作业3
用Qwen-LoRA方法，微调一个识别模型，数据集参考：05_BERT知识问答.pt

# 作业4
了解docker，推荐本地安装下， 后序方便部署dify