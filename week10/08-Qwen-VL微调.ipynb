{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51e30ff-d2cf-49cd-a230-b800af4143c7",
   "metadata": {},
   "source": [
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e149094e-4de0-4b4b-8c0e-8492caf53dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX_OCR目录已存在,跳过数据处理步骤\n"
     ]
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "from modelscope.msdatasets import MsDataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "MAX_DATA_NUMBER = 1000\n",
    "dataset_id = 'AI-ModelScope/LaTeX_OCR'\n",
    "subset_name = 'default'\n",
    "split = 'train'\n",
    "\n",
    "dataset_dir = 'LaTeX_OCR'\n",
    "csv_path = './latex_ocr_train.csv'\n",
    "\n",
    "# 检查目录是否已存在\n",
    "if not os.path.exists(dataset_dir):\n",
    "    # 从modelscope下载COCO 2014图像描述数据集\n",
    "    ds =  MsDataset.load(dataset_id, subset_name=subset_name, split=split)\n",
    "    print(len(ds))\n",
    "    # 设置处理的图片数量上限\n",
    "    total = min(MAX_DATA_NUMBER, len(ds))\n",
    "\n",
    "    # 创建保存图片的目录\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # 初始化存储图片路径和描述的列表\n",
    "    image_paths = []\n",
    "    texts = []\n",
    "\n",
    "    for i in range(total):\n",
    "        # 获取每个样本的信息\n",
    "        item = ds[i]\n",
    "        text = item['text']\n",
    "        image = item['image']\n",
    "        \n",
    "        # 保存图片并记录路径\n",
    "        image_path = os.path.abspath(f'{dataset_dir}/{i}.jpg')\n",
    "        image.save(image_path)\n",
    "        \n",
    "        # 将路径和描述添加到列表中\n",
    "        image_paths.append(image_path)\n",
    "        texts.append(text)\n",
    "        \n",
    "        # 每处理50张图片打印一次进度\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Processing {i+1}/{total} images ({(i+1)/total*100:.1f}%)')\n",
    "\n",
    "    # 将图片路径和描述保存为CSV文件\n",
    "    df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'text': texts,\n",
    "    })\n",
    "\n",
    "    # 将数据保存为CSV文件\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f'数据处理完成，共处理了{total}张图片')\n",
    "\n",
    "else:    \n",
    "    print(f'{dataset_dir}目录已存在,跳过数据处理步骤')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c46647-1445-47ef-998e-d6d7cba9f4dd",
   "metadata": {},
   "source": [
    "## 划分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09fd96a9-a1b8-4254-ac18-0f34162fc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "csv_path = './latex_ocr_train.csv'\n",
    "train_json_path = './latex_ocr_train.json'\n",
    "val_json_path = './latex_ocr_val.json'\n",
    "df = pd.read_csv(csv_path)\n",
    "# Create conversation format\n",
    "conversations = []\n",
    "\n",
    "# Add image conversations\n",
    "for i in range(len(df)):\n",
    "    conversations.append({\n",
    "        \"id\": f\"identity_{i+1}\",\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"value\": f\"{df.iloc[i]['image_path']}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\", \n",
    "                \"value\": str(df.iloc[i]['text'])\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# print(conversations)\n",
    "# Save to JSON\n",
    "# Split into train and validation sets\n",
    "train_conversations = conversations[:-4]\n",
    "val_conversations = conversations[-4:]\n",
    "\n",
    "# Save train set\n",
    "with open(train_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_conversations, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save validation set \n",
    "with open(val_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_conversations, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4173089-2810-4f89-8e14-17b3f23062b8",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee38ca54-a81e-432f-b61d-de8618277140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e97b32d3054f3284e0dc969a0346bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, TaskType, get_peft_model, PeftModel\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoProcessor,\n",
    ")\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# 使用Transformers加载模型权重\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model/Qwen2.5-VL-7B-Instruct/\", use_fast=False, trust_remote_code=True)\n",
    "# 特别的，Qwen2-VL-2B-Instruct模型需要使用Qwen2VLForConditionalGeneration来加载\n",
    "origin_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"./model/Qwen2.5-VL-7B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True,)\n",
    "origin_model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n",
    "processor = AutoProcessor.from_pretrained(\"./model/Qwen2.5-VL-7B-Instruct/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122648ad-4d74-4554-9d58-87792d634b91",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef223972-4cc8-4954-8720-c6c69f35c683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54cf5201aee4204a6bf61525ba846ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe841a4e67e34d2ab0abbba88066e471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='104' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [104/124 12:36 < 02:28, 0.13 it/s, Epoch 1.64/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 144\u001b[0m\n\u001b[1;32m    136\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    137\u001b[0m     model\u001b[38;5;241m=\u001b[39mtrain_peft_model,\n\u001b[1;32m    138\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    139\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    140\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForSeq2Seq(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# 开启模型训练\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2547\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2551\u001b[0m )\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2553\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2556\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2559\u001b[0m ):\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2561\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3718\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\n\u001b[1;32m   3699\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: nn\u001b[38;5;241m.\u001b[39mModule, inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]], num_items_in_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3700\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3702\u001b[0m \u001b[38;5;124;03m    Perform a training step on a batch of inputs.\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3716\u001b[0m \u001b[38;5;124;03m        `torch.Tensor`: The tensor with training loss on this batch.\u001b[39;00m\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3718\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   3720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2430\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2430\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2430\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2430\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.train at line 2430 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2430\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2430\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2429\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m-> 2429\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2319\u001b[0m, in \u001b[0;36mModule.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules.\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m \n\u001b[1;32m   2316\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   2317\u001b[0m \u001b[38;5;124;03m        Module: a child module\u001b[39;00m\n\u001b[1;32m   2318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2337\u001b[0m, in \u001b[0;36mModule.named_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \n\u001b[1;32m   2325\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \n\u001b[1;32m   2335\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2336\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m-> 2337\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   2339\u001b[0m         memo\u001b[38;5;241m.\u001b[39madd(module)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = \"你是一个LaText OCR助手,目标是读取用户输入的照片，转换成LaTex公式。\"\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "local_model_path = \"./Qwen/Qwen2-VL-2B-Instruct\"\n",
    "train_dataset_json_path = \"latex_ocr_train.json\"\n",
    "val_dataset_json_path = \"latex_ocr_val.json\"\n",
    "output_dir = \"./output/Qwen2-VL-2B-LatexOCR\"\n",
    "MAX_LENGTH = 8192\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    将数据集进行预处理\n",
    "    \"\"\"\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    conversation = example[\"conversations\"]\n",
    "    image_file_path = conversation[0][\"value\"]\n",
    "    output_content = conversation[1][\"value\"]\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": f\"{image_file_path}\",\n",
    "                    \"resized_height\": 500,\n",
    "                    \"resized_width\": 100,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )  # 获取文本\n",
    "    image_inputs, video_inputs = process_vision_info(messages)  # 获取数据数据（预处理过）\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = {key: value.tolist() for key, value in inputs.items()} #tensor -> list,为了方便拼接\n",
    "    instruction = inputs\n",
    "\n",
    "    response = tokenizer(f\"{output_content}\", add_special_tokens=False)\n",
    "\n",
    "\n",
    "    input_ids = (\n",
    "            instruction[\"input_ids\"][0] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    )\n",
    "\n",
    "    attention_mask = instruction[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n",
    "    labels = (\n",
    "            [-100] * len(instruction[\"input_ids\"][0])\n",
    "            + response[\"input_ids\"]\n",
    "            + [tokenizer.pad_token_id]\n",
    "    )\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    labels = torch.tensor(labels)\n",
    "    inputs['pixel_values'] = torch.tensor(inputs['pixel_values'])\n",
    "    inputs['image_grid_thw'] = torch.tensor(inputs['image_grid_thw']).squeeze(0)  #由（1,h,w)变换为（h,w）\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels,\n",
    "            \"pixel_values\": inputs['pixel_values'], \"image_grid_thw\": inputs['image_grid_thw']}\n",
    "\n",
    "\n",
    "def predict(messages, model):\n",
    "    # 准备推理\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # 生成输出\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=MAX_LENGTH)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    \n",
    "    return output_text[0]\n",
    "\n",
    "origin_model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n",
    "\n",
    "# 处理数据集：读取json文件\n",
    "train_ds = Dataset.from_json(train_dataset_json_path)\n",
    "train_dataset = train_ds.map(process_func)\n",
    "\n",
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,  # 训练模式\n",
    "    r=64,  # Lora 秩\n",
    "    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.05,  # Dropout 比例\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# 获取LoRA模型\n",
    "train_peft_model = get_peft_model(origin_model, config)\n",
    "\n",
    "# 配置训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=10,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# 配置Trainer\n",
    "trainer = Trainer(\n",
    "    model=train_peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "# 开启模型训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88586f3-a5be-453b-831e-567b69a6e32e",
   "metadata": {},
   "source": [
    "多模态大模型微调进行目标检测：\n",
    "\n",
    "训练数据：图片的输入、物体框（json）文本\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19f0f4-999f-41be-996d-ccaa1499589a",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ea842b-5f28-4e1f-9075-dbf686cdf4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_path: ./output/Qwen2-VL-2B-LatexOCR/checkpoint-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====================测试===================\n",
    "# 配置测试参数\n",
    "val_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=True,  # 训练模式\n",
    "    r=64,  # Lora 秩\n",
    "    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.05,  # Dropout 比例\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# 获取测试模型，从output_dir中获取最新的checkpoint\n",
    "load_model_path = f\"{output_dir}/checkpoint-{max([int(d.split('-')[-1]) for d in os.listdir(output_dir) if d.startswith('checkpoint-')])}\"\n",
    "print(f\"load_model_path: {load_model_path}\")\n",
    "val_peft_model = PeftModel.from_pretrained(origin_model, model_id=load_model_path, config=val_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd68dec6-6b8a-497d-ba11-325cfe036ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:d s ^ { 2 } = ( 1 - \\frac { q c o s \\theta } { r } ) _ { 1 + \\alpha ^ { 2 } } ^ { 2 } \\{ d r ^ { 2 } + r ^ { 2 } d \\theta ^ { 2 } + r ^ { 2 } s i n ^ { 2 } \\theta d \\varphi ^ { 2 } \\} - \\frac { d l ^ { 2 } } { ( 1 - \\frac { q c o s \\theta } { r } ) _ { 1 + \\alpha ^ { 2 } } ^ { 2 } } .\n",
      "gt:d s ^ { 2 } = ( 1 - { \\frac { q c o s \\theta } { r } } ) ^ { \\frac { 2 } { 1 + \\alpha ^ { 2 } } } \\lbrace d r ^ { 2 } + r ^ { 2 } d \\theta ^ { 2 } + r ^ { 2 } s i n ^ { 2 } \\theta d \\varphi ^ { 2 } \\rbrace - { \\frac { d t ^ { 2 } } { ( 1 - { \\frac { q c o s \\theta } { r } } ) ^ { \\frac { 2 } { 1 + \\alpha ^ { 2 } } } } } .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_image_list = []\n",
    "for item in train_dataset:\n",
    "    image_file_path = item[\"conversations\"][0][\"value\"]\n",
    "    label = item[\"conversations\"][1][\"value\"]\n",
    "    \n",
    "    messages = [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [\n",
    "            {\n",
    "            \"type\": \"image\", \n",
    "            \"image\": image_file_path,\n",
    "            \"resized_height\": 100,\n",
    "            \"resized_width\": 500,   \n",
    "            },\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt,\n",
    "            }\n",
    "        ]}]\n",
    "    \n",
    "    response = predict(messages, val_peft_model)\n",
    "    \n",
    "    print(f\"predict:{response}\")\n",
    "    print(f\"gt:{label}\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369a3c5-06ab-4d94-a669-d735fdf59721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
