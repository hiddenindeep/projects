{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63bf0817-f439-4283-b8db-54784558176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"Vary-toy/Vary-master/\")\n",
    "os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'\n",
    "\n",
    "# pip install albumentations==1.4.8 albucore==0.0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "893d16f5-27e0-4e6b-a598-2d4be633c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.5 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from vary.utils.conversation import conv_templates, SeparatorStyle\n",
    "from vary.utils.utils import disable_torch_init\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\n",
    "from vary.model import *\n",
    "from vary.utils.utils import KeywordsStoppingCriteria\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from vary.model.plug.blip_process import BlipImageEvalProcessor\n",
    "from transformers import TextStreamer\n",
    "from vary.model.plug.transforms import train_transform, test_transform\n",
    "\n",
    "\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = '<imgpad>'\n",
    "DEFAULT_IM_START_TOKEN = '<img>'\n",
    "DEFAULT_IM_END_TOKEN = '</img>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c32e91-6f65-4f9a-960f-b5b70cff6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9f89ca-f1e1-4e68-9c61-bc1fe3ee4f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mmgpt to instantiate a model of type vary. This is not supported for all configurations of models and can yield errors.\n",
      "QWenLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "varyQwenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "QWenLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "varyQwenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "QWenLMHeadModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "varyQwenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "varyQwenForCausalLM(\n",
       "  (transformer): varyQwenModel(\n",
       "    (wte): Embedding(151860, 2048)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x QWenBlock(\n",
       "        (ln_1): RMSNorm()\n",
       "        (attn): QWenAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): RMSNorm()\n",
       "        (mlp): QWenMLP(\n",
       "          (w1): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (w2): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (c_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "    (vision_tower): CLIPVisionModel(\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (vision_tower_high): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "      (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (mm_projector): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (mm_projector_vary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151860, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "disable_torch_init()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/model/Vary-toy\", trust_remote_code=True)\n",
    "model = varyQwenForCausalLM.from_pretrained(\"/root/autodl-tmp/model/Vary-toy\", low_cpu_mem_usage=True, trust_remote_code=True)\n",
    "model.to(device='cuda',  dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c86523-0c59-442f-8bd9-0462913b5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = CLIPImageProcessor.from_pretrained('/root/autodl-tmp/model/clip-vit-large-patch14/', torch_dtype=torch.float16)\n",
    "image_processor_high = BlipImageEvalProcessor(image_size=1024)\n",
    "use_im_start_end = True\n",
    "image_token_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df107ba3-f0e5-4c9f-911e-7c9bf012c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vary(prompt: str, image_path: str):\n",
    "    qs = prompt\n",
    "    if use_im_start_end:\n",
    "        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN*image_token_len + DEFAULT_IM_END_TOKEN  + '\\n' + qs\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n",
    "    \n",
    "    \n",
    "    conv_mode = \"mpt\"\n",
    "    \n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    \n",
    "    \n",
    "    inputs = tokenizer([prompt])\n",
    "    \n",
    "    image = load_image(image_path)\n",
    "    image_1 = image.copy()\n",
    "    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "    \n",
    "    image_tensor_1 = image_processor_high(image_1)\n",
    "    \n",
    "    input_ids = torch.as_tensor(inputs.input_ids).cuda()\n",
    "    \n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=[(image_tensor.unsqueeze(0).half().cuda(), image_tensor_1.unsqueeze(0).half().cuda())],\n",
    "            do_sample=True,\n",
    "            num_beams = 1,\n",
    "            # temperature=0.2,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=2048,\n",
    "            stopping_criteria=[stopping_criteria]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff984634-a84e-4238-a168-1d0a37d12d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# \n",
      "å°ç»“\n",
      "\n",
      "## å‘é‡æ£€ç´¢çš„æŒ‘æˆ˜ä¸å®è·µ\n",
      "\n",
      "- æ•°æ®è§„æ¨¡å¤§, ä¿¡æ¯å¯†åº¦é«˜, å¤„ç†æˆæœ¬é«˜\n",
      "- å‘é‡æ£€ç´¢ã€RAG éœ€æ±‚å¿«é€Ÿå¢é•¿\n",
      "å‘é‡æ£€ç´¢çš„æ–°CAPé—®é¢˜\n",
      "\\(\\cdot\\) åœ¨æˆæœ¬ã€ç²¾åº¦ã€æ€§èƒ½ä¹‹é—´å–èˆ\n",
      "å‘é‡æ•°æ®åº“å®è·µ\n",
      "- å­˜å‚¨å·¥ç¨‹ä¸å‘é‡ç´¢å¼•ç®—æ³•æ·±åº¦ä¼˜åŒ–\n",
      "ãƒ»é€šè¿‡æ··åˆç£ç›˜ç´¢å¼•é™ä½æˆæœ¬\n",
      "- è®©åº”ç”¨ç²¾é€šå‘é‡æå‡å¬å›ç‡\n",
      "æœ€æ–°å­¦æœ¯ç ”ç©¶ä¸åº”ç”¨\n",
      "\\(\\cdot\\) RabinQ è¶…é«˜é‡åŒ–æ¯”\n",
      "HGraphå±‚æ¬¡åŒ–ç´¢å¼•æ¡†æ¶\n",
      "- æä¾›çµæ´»ç»„åˆèƒ½åŠ›ï¼Œå¿«é€Ÿæ„å»ºæ–°ç´¢å¼•\n",
      "ç£ç›˜ç´¢å¼•ä¸Šçš„æ”¹è¿›\n",
      "- PAGE è¿›ä¸€æ­¥é™ä½å†…å­˜å’Œ \\(I O\\) éœ€æ±‚\n",
      "åŸºäºå…¬å¼€ Benchmark å·¥å…·çš„æ€§èƒ½è°ƒä¼˜\n",
      "- GIST-960æ•°æ®é›†ä¸šç•Œ SOTA\n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Cnvert the image to latex format:\", \"data/iShot_2025-04-23_16.31.41.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "296b5750-36b6-4bb4-acb0-4f35e303015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å»ºè®¾åœ°ç‚¹\n",
      "\n",
      "å»ºè®¾è§„æ¨¡ \\(1-7 \\mathrm{~h}\\)\n",
      "åœŸæ–¹æ¦‚è§„æ¨¡ 893mÂ³ã€åˆå \\(62\\%\\)\n",
      "\n",
      "åœŸå±‚\n",
      "æ´»åŠ¨åŒºç§¯æ²™å¤§æ”¾:\n",
      "è´¨æ¥æº ç”µ\n",
      "è¯ (883)ä¸€817715\n",
      "æ§åˆ¶æŠ•èµ„ï¼šæ‹ŸæŠ•,\n",
      "å»ºç­‘é¢ç§¯\n",
      "æ­£å¸¸è®¾è®¡ä¾æ®:ï¼ˆè¯·å¡«æŠ¥æ˜æ‰¹æ–‡çš„ç¥¨è¦å†…å®¹åŠæ‰¹æ–‡å·ï¼‰ \n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Cnvert the image to markdown format:\", \"data/xh44z6ajf6noc_22c15d2af82e42a290dac1dde66bc685.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "696809eb-605e-43c9-b1ec-918ff27a752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[720, 126, 938, 635]\n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Detect the person in the image\", \"data/pingpong.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6e7113b-5db0-4752-9277-b306cbe6c342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two people playing ping pong in an indoor court.\n"
     ]
    }
   ],
   "source": [
    "run_vary(\"Describe the image\", \"data/pingpong.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce90c2-f2be-42aa-982a-7a3bc62f34e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
