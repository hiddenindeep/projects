全文索引:
    全文索引:将文档中的词与文档ID的映射关系存储起来
    全文索引的构建:
        1.将文档分词
        2.将每个词与文档ID的映射关系存储起来
        3.将所有词的映射关系存储起来
    全文索引的查询:
        1.将查询分词
        2.在倒排索引中查找每个词的文档ID
        3.将所有文档ID取交集，得到最终结果

倒排索引是实现全文索引最核心、最基础、最不可或缺的技术。
布尔索引（布尔检索）是倒排索引最直接、最经典的应用方式

    
TFIDF与BM25的区别:
TFIDF:
    TF:词频（Term Frequency） 表示一个词在文档中出现的频率
    IDF:逆文档频率  （Inverse Document Frequency） 表示一个词在所有文档中出现的频率
    TFIDF:词的权重 = TF * IDF
BM25:（可以看作tfidf的优化版）
    TF:词频
    IDF:逆文档频率  使用了饱和函数来计算IDF，平衡长文档对打分的影响
    k1:控制词频对BM25的影响
    b:控制文档长度对BM25的影响
    k1和b是调节参数，可以通过实验来调节,k1一般1.2-2.0,b一般0-1.0

RAG:基于检索的生成模型,增强检索生成
    1.将问题转化为检索任务
    2.使用检索模型检索相关文档
    3.使用生成模型生成答案


深度学习Deep Learning:包含多层神经网络，通过学习数据中的规律，自动提取特征，实现复杂任务
    端到端的计算，从输入到输出
    一个有向计算图，图的节点表示计算过程
    每个节点表示一个计算单元，每个边表示一个数据流
    每个节点有输入和输出，输入是前一个节点的输出
    每个节点有参数，参数通过学习得到
    每个节点有激活函数，激活函数将输入映射到输出
    每个节点有损失函数，损失函数衡量输出与目标之间的差异
    每个节点有优化器，优化器通过梯度下降法更新参数

神经元：神经网络的基本单元，由输入、权重、激活函数、输出组成
    输入：神经元的输入，可以是任意维度的向量
    权重：神经元的权重，与输入一一对应，可以是任意维度的向量
    激活函数：神经元的激活函数，将输入和权重相乘，得到中间结果，再通过激活函数得到输出
    输出：神经元的输出，可以是任意维度的向量

神经网络：主要由大量的神经元以及它们之间的有向连接构成。
神经元的激活规则：主要指神经元输入与输出之间的映射关系，一般为非线性函数
网络的拓扑结构：不同神经元的连接方式，一般分为全连接、局部连接等
网络的训练过程：通过梯度下降法，不断调整神经元的权重，使得网络的输出与目标之间的差异最小

output = activation_function(input * weight + bias)

前馈神经网络（Feed forward）:
    通过正向传播，从输入到输出做预测；然后通过反向传播和梯度下降法进行调整模型参数，不断的优化模型的参数，最后让模型的输出结果和真实结果之间的差异最小
    模型：y=f5(f4(f3(f2(f1(x)))))，其中f1,f2,f3,f4,f5都是激活函数,每一层基本使用同一个函数
    学习准则：L(y,y*) = 1/2*(y-y*)^2,其中y为预测值，y*为真实值  损失函数
    优化：梯度下降（链式法则）：通过计算损失函数的梯度，然后沿着梯度的反方向更新参数，使得损失函数的值不断减小

    激活函数：Sigmoid、Tanh、ReLU、Leaky ReLU、ELU、Softmax、Swish等
    损失函数：均方误差（MSE）、交叉熵（Cross Entropy）、均方对数误差（MSLE）、Huber Loss等. 非线性复杂的
    优化器：随机梯度下降（SGD）、Adam、RMSprop、Adagrad、Adadelta等

循环神经网络（Recurrent Neural Network）
图网络（Graph Neural Network）